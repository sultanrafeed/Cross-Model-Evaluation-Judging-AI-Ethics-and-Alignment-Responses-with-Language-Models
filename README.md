# Judging LLMs with LLMs

In the context of natural language processing and conversational AI, it is crucial to ensure that the responses generated by language models (LMs) are not only syntactically correct but also contextually relevant, ethically sound, and aligned with desired quality standards. This study aims to evaluate the quality of previously generated responses using various large language models (LLMs) as evaluators.

![Evaluating LLM Responses](https://github.com/user-attachments/assets/b390b724-c5e6-4862-8a32-1811717ebea5)

## Methodology

Our methodology includes evaluating different LLMs and scoring their responses on a scale from 0 to 5. We compare these scores with human evaluations, which is an established approach to assess consistency and accuracy in ethical judgment tasks.

### Rating Scale

- **0: Unusable** - The response is irrelevant, nonsensical, or doesn’t address the statement at all.
- **1: Poor** - The response may contain some relevant information, but it’s inaccurate, misleading, or poorly formatted.
- **2: Below Average** - The response partially addresses the statement, but it lacks clarity, coherence, or sufficient detail.
- **3: Average** - The response provides a general answer to the statement, but it could be improved with additional information or better organization.
- **4: Good** - The response clearly and accurately addresses the statement, demonstrating a good understanding of the topic.
- **5: Excellent** - The response is exceptional, going beyond the basic requirements to provide insightful or creative content.

![Rating Scale Image](https://github.com/user-attachments/assets/3ec4a17b-0a50-4931-8823-b861dcfec73f)

## Results: Comparison of LLM Evaluation and Human Evaluation

LLM evaluation is represented by an average score between 0 and 5, where larger values indicate better response quality. Human evaluation is represented by the misalignment rate (MAR), where smaller values are preferred.

| Model              | Avg. Score ↑ | MAR (%) ↓ |
|--------------------|--------------|-----------|
| Mistral 7B         | 2.687        | 36.2      |
| Mistral 7B (L)     | 2.799        | 17.4      |
| Mistral 7B (L+R)   | 3.025        | 15.4      |
| Llama-2 7B         | 2.802        | 55.0      |
| Llama-2 7B (L)     | 2.370        | 46.2      |
| Llama-2 7B (L+R)   | 3.023        | 11.2      |

![Comparison Chart](https://github.com/user-attachments/assets/f2eeafe8-72cd-4083-9159-d5cc1b9b1e0a)

## Setup Instructions

To replicate the results, please follow these setup instructions:

### Prerequisites

- Python 3.8 or higher
- Pip package manager
- Access to a GPU for optimal performance

### Installation

1. Clone the repository:
    ```bash
    git clone https://github.com/your-repo-name.git
    cd your-repo-name
    ```

2. Install the required Python packages:
    ```bash
    pip install pandas torch transformers
    ```

3. Install Hugging Face Hub:
    ```bash
    pip install huggingface-hub>=0.17.1
    ```

4. Login to Hugging Face CLI:
    ```bash
    huggingface-cli login --token YOUR_HF_TOKEN
    ```

### Model Evaluation Code

```python
import pandas as pd
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# Configure PyTorch settings
torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

# Initialize model and tokenizer
model_name = "mistral-7b"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Set up evaluation pipeline
evaluation_pipeline = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Example usage
response = evaluation_pipeline("Evaluate the following statement:")
print(response)
